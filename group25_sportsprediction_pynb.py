# -*- coding: utf-8 -*-
"""Group25_SportsPrediction.pynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1FdzPjObfhhGrjUunshjPjYWeQgTII6hC

Importing all needed libraries
"""

import pickle as pk
import xgboost as xgb
from google.colab import drive
drive.mount('/content/drive')
import numpy as np
import pandas as pd
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split,GridSearchCV,cross_val_predict, KFold
from sklearn.ensemble import RandomForestRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import GradientBoostingRegressor
from xgboost import XGBRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error

"""Reading the data from CSV files"""

data = pd.read_csv('/content/drive/My Drive/Colab Notebooks/Intro to AI/players_21.csv')

dataset = pd.read_csv('/content/drive/My Drive/Colab Notebooks/Intro to AI/players_22.csv')

"""Displaying the first 5 rows of the data to have an idea of what the data looks like

---


"""

data.head()

"""Checking the type of values for the 'overall' column

----

"""

data['overall']

"""Creating a list of columns to encode the categorical features using one-hot encoding in the dataset and concatenating with to the numerical data."""

columns_to_encode = []
for column in data.columns:
  if data[column].dtype == object:
    columns_to_encode.append(column)
# Use get_dummies with dummy_na=True
df_encoded= pd.get_dummies(columns_to_encode, prefix = "encoded_", dummy_na=True)
data.drop(columns_to_encode, axis = 1, inplace = True)
data = pd.concat([data, df_encoded], axis=1)

"""Iterating through each column in the DataFrame 'data' and determining whether the percentage of missing values in the column exceeds 30% of the total number of rows. If it exceeds the threshold, remove the column from the 'data' DataFrame.This technique is used to delete columns that have a significant percentage of missing data and may be useless for analysis.

"""

for column in data.columns:
    if data[column].isna().sum() > 0.3 * len(data):
        data.drop(column, axis=1, inplace=True)

"""Storing the list of column names"""

columns = data.columns

"""Create a SimpleImputer object called 'imputer

"""

imputer = SimpleImputer()

"""Converted column names to strings for consistency"""

data.columns = data.columns.astype(str)

"""Imputing the data to handle missing values

"""

imputer.fit(data)
data =imputer.fit_transform(data)
data = pd.DataFrame(data, columns = columns)

"""Creating new subset columns in data by calculating the mean of specific groups of columns by aggregating related features into average values for further analysis or modeling."""

data['goalkeeping_average'] = data[['goalkeeping_diving', 'goalkeeping_handling', 'goalkeeping_kicking', 'goalkeeping_positioning', 'goalkeeping_reflexes']].mean(axis =1)
data['attacking'] = data[['attacking_crossing', 'attacking_finishing', 'attacking_heading_accuracy',
                                'attacking_short_passing', 'attacking_volleys']].mean(axis =1)
data['defending_average'] = data[['defending_marking_awareness', 'defending_standing_tackle', 'defending_sliding_tackle' ]].mean(axis =1)
data['skill'] = data[['skill_dribbling','skill_curve','skill_fk_accuracy','skill_long_passing','skill_ball_control']].mean(axis =1)
data['power_average'] = data[['power_shot_power', 'power_jumping', 'power_stamina',
                                'power_strength', 'power_long_shots' ]].mean(axis =1)
data['movement_average'] = data[['movement_acceleration', 'movement_sprint_speed', 'movement_agility',
                                'movement_reactions', 'movement_balance']].mean(axis =1)
data['goalkeeping_average'] = data[['goalkeeping_diving', 'goalkeeping_diving', 'goalkeeping_kicking',
                                'goalkeeping_positioning', 'goalkeeping_reflexes','goalkeeping_handling']].mean(axis =1)
data['wage_to_value_ratio'] = data['value_eur'] / data['wage_eur']

"""Using correlation to determine the features that have a strong relationship
with the target("overall")
"""

correlations = data.corrwith(data['overall'])
correlation_pairs = list(zip(data.columns, correlations)) # A list of tuples containing column names and their correlation coefficients with the target variable.

"""This code filters columns from the DataFrame 'data' based on their correlation with the target variable.
It removes columns where the correlation coefficient with the target variable is less than 0.4.
"""

for item in correlation_pairs:
  column_name, value = item
  if value <0.4:
    data.drop([column_name],axis =1, inplace = True)

"""Dropping the features that are repeated. These features individual have a correlation above 0.4 with the target variable. However, the subset feature has a better relation with the target."""

data.drop(['skill_curve', 'skill_long_passing', 'skill_ball_control','attacking_crossing', 'attacking_short_passing','power_shot_power',
           'power_long_shots'], axis = 1, inplace = True)

data.columns

"""Scaling the data using MinMaxScaler"""

y = data['overall']
x = data.drop('overall', axis = 1)

scaler = MinMaxScaler()
columns_x = x.columns
x = scaler.fit_transform(x)
x =pd.DataFrame(x, columns = columns_x)

"""Saving the scaler model for future use

"""

with open('/content/drive/MyDrive/scaler_model2.pkl', 'wb') as scaler_file:
    pk.dump(scaler, scaler_file)

"""Defining the learning models that will be trained"""

rfr = RandomForestRegressor()
dtr = DecisionTreeRegressor()
gbr = GradientBoostingRegressor()
xgb= XGBRegressor()

models = {
    "Random Forest": rfr,
    "Decision Tree": dtr,
    "Gradient Boost": gbr,
    "XGBoost": xgb
}
mae_score= {} # a dictionary to store the models and a list of their mean absolute error

"""Set up k-fold cross-validation with 5 folds, shuffling the data, and using a random state of 42.Storing mean absolute error values in a list to keep results from cross validation."""

# Set up k-fold cross-validation
kfold = KFold(n_splits=5, shuffle=True, random_state=42)

"""Loop through different models and perform k-fold cross-validation for each model, then split the data into training and testing sets for each fold using 'kfold'
Fit the current model on the training data
Make predictions on the test data
Calculate Mean Absolute Error (MAE) between true and predicted values
Store the MAE scores in the'mae_scores', for each model
Store the final MAE scores for each model in the 'mae_score' dictionary

"""

for model_name, model in models.items():
  # Lists to store the cross-validation results
    mae_scores = []
    for train_index, test_index in kfold.split(x):
        X_train, X_test = x.iloc[train_index], x.iloc[test_index]
        Y_train, Y_test = y.iloc[train_index], y.iloc[test_index]

        # Fit the model on the training data
        model.fit(X_train, Y_train)

        # Make predictions on the test data
        y_pred = model.predict(X_test)

        # Calculate mean squared error and store it
        mae = mean_absolute_error(Y_test, y_pred)
        mae_scores.append(mae)
    mae_score[model_name] = mae_scores

"""Display the mean absolute error for each model"""

print(mae_score)

"""Finding the best model with the lowest mean absolute error 'min_error' is initialized to positive infinity and 'best_model' to None.
Loop through the models and calculate the mean error for each using k-fold cross-validation. If the mean error for this model is smaller than the current minimum error, update the best model and min_error.
"""

min_error = float('inf')
best_model = None

for model_name, error_list in mae_score.items():
    mean_error = sum(error_list) / len(error_list)

    if mean_error < min_error:
        min_error = mean_error
        best_model = model_name

"""Store the name of the best model."""

model = models[best_model]

"""Display the columns of the 'data' dataframe."""

data.columns

"""Display summary statistics for the 'data' dataframe."""

data.describe() # description of features

"""Splitting the data into training and testing sets."""

x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)

"""Fine tuning the gradient boosting regressor, performed grid search with cross-validation to find the best hyperparameters for the model.Retrieved the best model with the optimal hyperparameters then evaluated tofine tuned model on the test set"""

param_grid = {'n_estimators': [50, 100, 200], 'max_depth': [None, 10, 20]}

# Assuming 'model' and 'param_grid' are defined earlier
grid_search = GridSearchCV(model, param_grid, cv=5, scoring='neg_mean_absolute_error')
grid_search.fit(x_train,y_train)

# Now, you can use these best parameters to train your model on the entire training set or do further analysis
best_model = grid_search.best_estimator_
best_model.fit(x_train, y_train)

# Evaluate the model on the test set
y_pred = best_model.predict(x_test)
mae = mean_absolute_error(y_test, y_pred)
print(f'Mean Absolute Error on Test Set: {mae}')

grid_search.best_params_

print(best_model)

"""Created an empty dictionary to store the best hyperparameters."""

best_params_grid = {}

"""Display the minimum error value calculated earlier.

"""

min_error

"""Checking if the tuned model is better than the actual mode:We check whether the Mean absolute error of the tuned model is smaller than that of the original"""

mae < min_error

"""Display the columns of the 'data' dataframe."""

data.columns

"""Saving the best model in a file"""

file_name = "/content/drive/My Drive/Colab Notebooks/best_model2.pkl"
with open(file_name, "wb") as file:
  pk.dump(best_model,file)

"""Testing my model with Player22.
Loop through the dataset and remove columns from the 'dataset' dataframe with too many missing values.
"""

for column in dataset.columns:
    if dataset[column].isna().sum() > 0.3 * len(data):
        dataset.drop(column, axis=1, inplace=True)

"""To determine which columns in the 'dataset' DataFrame have a data type of 'object' (which usually indicates categorical variables).
The names of these categorical columns should be saved in the 'columns_to_encode' list.
To handle missing data, use the 'pd.get_dummies' function to one-hot encode the category columns with 'dummy_na=True'.
Along the columns (axis=1), concatenate the one-hot encoded columns with the original 'dataset'.
Using 'drop', implace, and axis=1, remove the original categorical columns from 'dataset'.
"""

columns_to_encode = []
for column in dataset.columns:
  if dataset[column].dtype == object:
    columns_to_encode.append(column)
# Use get_dummies with dummy_na=True
df_encoded= pd.get_dummies(columns_to_encode, columns=columns_to_encode, dummy_na=True)
dataset = pd.concat([dataset, df_encoded], axis=1)
dataset.drop(columns_to_encode, axis = 1, inplace = True)

"""Ensure all columns are of string data type"""

dataset.columns = dataset.columns.astype(str)
columns = dataset.columns

"""Fitting the imputer to the 'dataset' and imputing missing values."""

imputer.fit(dataset)
dataset =imputer.fit_transform(dataset)
dataset= pd.DataFrame(dataset, columns = columns)

"""Create new columns in 'dataset' by calculating the mean of specific groups of columns, similar to what was done for the 'data' dataset."""

dataset['goalkeeping_average'] = dataset[['goalkeeping_diving', 'goalkeeping_handling', 'goalkeeping_kicking', 'goalkeeping_positioning', 'goalkeeping_reflexes']].mean(axis =1)
dataset['attacking'] = dataset[['attacking_crossing', 'attacking_finishing', 'attacking_heading_accuracy',
                                'attacking_short_passing', 'attacking_volleys']].mean(axis =1)
dataset['defending_average'] = dataset[['defending_marking_awareness', 'defending_standing_tackle', 'defending_sliding_tackle' ]].mean(axis =1)
dataset['skill'] = dataset[['skill_dribbling','skill_curve','skill_fk_accuracy','skill_long_passing','skill_ball_control']].mean(axis =1)
dataset['power_average'] = dataset[['power_shot_power', 'power_jumping', 'power_stamina',
                                'power_strength', 'power_long_shots' ]].mean(axis =1)
dataset['movement_average'] = dataset[['movement_acceleration', 'movement_sprint_speed', 'movement_agility',
                                'movement_reactions', 'movement_balance']].mean(axis =1)
dataset['goalkeeping_average'] = dataset[['goalkeeping_diving', 'goalkeeping_diving', 'goalkeeping_kicking',
                                'goalkeeping_positioning', 'goalkeeping_reflexes','goalkeeping_handling']].mean(axis =1)

"""Filtering the 'dataset' to keep only the features used to train the model."""

columns_to_keep = data.columns
dataset = dataset[columns_to_keep]

"""Scaling the test data using the previous MinScaler"""

y= dataset['overall']
x = dataset.drop('overall', axis =1)
columns_x = x.columns
x = scaler.fit_transform(x)
x =pd.DataFrame(x, columns = columns_x)

"""load the model from the file."""

loaded_model = pk.load(open(file_name, 'rb'))

"""Use the loaded model to make predictions on the test data."""

prediction = loaded_model.predict(x)

"""Calculate the mean absolute error of the predictions."""

mae = mean_absolute_error(y, prediction)
print(f"Mean Absolute Error: {mae}")